My conclusions during coding
12.03.2020

Goal was to write additional block to mass_algo_classif_function, to make Feature Selection base on the feat_importance.
From 8 models, startartize block managed to do only for 3-4 of them.
Reasons: KNN has no feat_importance, CATBoost, Gaussian and SVM.SVC as well required peculiar approach.
Conclusion: 
1. Make separate feature selection after model defined


23.03.2020

Finished and uploaded first draft of Corona_prediction

Future steps:
1. MOre models and tune
2. Check prediction
3. Prediction for more countries
4. Go through another Kernel.Check logic!


30.03

Preventive Maint-Model_Improved

1. Score improved by drop y=['ttf'] - 5% min and max
2. Why predicted is beter than av.Std is equal.
Recalculate to RSME and will see
3. Drop colin feat improved score (even best RF) (Done)
4. Graph for Predicted/Actual (Done)
5. Control Outliars by Z_score (Done)
6. If add cycle we predict better 25.7 instead 28.
How fair is it to keep this column?
7. z=2,orig+cycle XGBOOST no sel=23.701404, RF_Impr=23.607516
7. QIR,orig+cycle bad

31.03

Instruction for Outliars Detection and Normality Check (Done)

01.04
Binary classification
1. Predict_proba (2 columns - probability to belong one of classes) and 
decision function (if no predict_proba, posotive- class 1, neg - class 0)

2. Why Predict_proba but not just prediction - used to define threshold at which we should switch from 0 to 1


02.04
Multiclass classification
1. Prepare y - transform to binary classificaton - LabelBinaliazer
2. Function with GridSearch to calc y_pred or y_score
3. Metrics calculare
4. Precision-Recall Curve to STUDY!

05.04

Lecture 2

1. Create test set from data with time element - without data Shuffle or random pick up.
Because usually new data is fresh one.

2. %time - if more that 10 sec - too long

3. Bagging - Random Forest - Ensemble of trees. 
Build 10 crappy trees, average them and get 1 good.

Different input for each tree (different subsent or data with replacement - bootstrap)

4. If score on valid is low but we dont know is it because valid test has another dataline or because train is overfitted,
we can solve it using __OUT_OF_BAG__ score.
Idea is we calculate train error on the rows from train set but that wasnt used in this tree fitting.
 So we dont need valid set and we can check how model generalize on the new data
 
 5. RF tunning - min_sample_leaf - stop split node if <=min_sample_leaf
 6. RF tunning - max_features -
 
 Idea to have less correlated trees.
 If you have so good columns, each tree starts with splitting this column.
 With this tune we are looking for splitting not to all columns but to subset of them (50 %, 70%, sqrt, log)
  
  
  
  - stop split node if <=min_sample_leaf
  
  09.04.2020
  1. Feature Importance - if too high probably colinear and try model with low features
  2. 2 Reasons why score on valid not so good as train one:
   - Overfitting, probably you can recheck it by creating addiyional valid set or to use oob (if set is big),
   if it is as well has a big error - 100% you overfit
   
   - if your oob not bad - you are doing someething wrong in your dataset
   if your dataset is not random, f.e. data for last 2 weeks and it doesnt work good,
   could be that something happend in last 2 weeks. In this case you can break you valid dataset
   
   3. One-Hot Enc
   
    - encode categ data
    - drop first col for linear models because they are hate colinear
    - dont drop it for RF because we dont care much about colin here
    
   even if it is not much improve your model, do one-hot and define feat.import
   this will help better explain what feat infl on prediction
   
  how decide what columns from caterorical to one-hot?
  If cardinality less than 7 please one  hot -example
  
  dummy=One_hot
  
  4. Removing REbundat features
   - Use hierarchy- dendrogram - cluster analysis
   Clustering
   1. KNN
   2. Hierarchical or Aglomerating
    - draw all points, consider in parallel by 2, replace 2 nearest by average and continue
    
    in dendrogramm more close commom line to right (to features) more similar yhey are
    
    - To build dendrogr we have to be sure no NAN in corr matrix.
    For this ddrop all feat with feat importance 0
    
    10.04.2020
    
    Validation is worse that train
    Check is dates make influence in validation 
    1. Check oob=True score, if it is good and validation is bad it means that valid set time make influence and model overfitted
    
    Avoid overfitting
    
    1. Subsampling
    2. Set up limit amount of samples per tree
    3. Tree tunning parameters
     - prevent overfitting by making less deep tree:
      - min_samples_leaf -stop to split node at this point
      - max_features - limit columns 
      
    4. More modelling you dont need all ds for each tree, 
    using subset show both faster and provide better interpretation - less overfit
    
    5. We made prediction, but how check confidence - use st dev or st dev/predicted price
    
    Partial dependence 
    From the important features (feat impot) how feature relate to y
    we take ds and  values in column YearMade replace to 1 of values f.e.1990
    And push this ds through already prepared model and defines relations
    
    Here we can see some trends and clusrers of lines. Some product are growing some still flat
    Just a way to better undestand data
    
    5. Tree Interpreter
    
    If someone ask why we predicted this price we can use Treeinterpreter to explain

 1. We have row to predict
 2. we calcaulate predicted price
 3. Show bias - average of all prices - value before split
 4. contribution - negative and positive contr of features to predicted price
 
 GOOD ML Job - if we know how our model generalize otherwise it is not good ML JOB
 
 Check MOdel Generalization Skill
Random Sample as Valid Set
What if not Generilized on Valid. If we go back change few times and it worked probably occidentually Overfit valid set?

Have few valid set - valid+test If they are both ok, so you model good generalize!

Instead Valid can be used OOB

But in average it is less good than valid, because ....

SOmetimes we are not able to use random subset - f.e. in case of forecasring - timeelement here

No sense to check how accurate your model predict data with 4 years old timeline

As well validation set could be wrong - it imbalanced. Desease only 1 row

SO - in PM valid set should be not last one rows but random
How to buld good Valid set? What if OOB good but we fail in prediction future during valid set? Here maybe better consider not full train set but last 2 weeks

So

Fit model Train
Check oob
Check valid
Check valid+train
Apply test
SO - How check is your valid set is represential with TEST
- 1. fit 5 models with train
    - could be train on full ds, on last 2 weeks, etc.
- 2. find 4 scores with valid
- 3. 5 score with test
- 4. Build stright line chart - if scores are same - good representation

PM - how define valid set that represent test?

2 potential:

1. Random from each id
2. Random from all ds

3 models:
- full ds, first 10 000, last 10 000 (no sort, by id only, added cycle column (later try only corr col))

REsult: More close score Valid and TEst: Random row from all ds (valid-0.77,0.89,0.70 test-0.65,0.62,0.55, OOB- 0.75, 0.86,0.70)

12.04.2020

1.THE MOST important stepin ML - to be sure that your valid set is representative to TEST one!
2. WHY not always good to use Cross_validation:
 - long
 - it shuffle data and our valid set could have no sense in shuffle form
 3. What is it TREE-ENterpiter?
 - explains how each feature contributes (neg and positivly) to final prediction price 
 - like from average price, for point 1 the feature 'Grarage_1' add 0.1 - USE WATERFALL Chart
 - Example of Tree-Enterp - Explain me why your model shows that I will like this crappy movie?
  - see because you like that similar movie and you that age range....
 -bias+contribution = predicted price
 
 4. EXTRAPOLATION
 
 If you need to predict price next month, linear regression will draw the line over time.
 It is difficult for RF, there are few ways how to deal
  - if there are other dependencies, escape usinf dateline in RF
   1. What is the difference between valid and test set?
    - combine again train/valid set or train/test, create column 1/0 is row in valid set
    - now y = ['is_valid']
    - throught RF try to predict it
    - if you can predict that means your (test set that your conbinned with train ) is not random and think how yuo can create valid set
    - check feat import. If there f.e. Sale_Time - probably test set is just sets later sold, machine_id - is just machine's number that was sold later
    - grad first 2 import feat and .describe info and you can check mean in train/valid/test or размах
    - drop first 3 feat, again calc fi
    - create list with time-dep feat, drop by one and check score
    - define which to drop
    - drop group define score

13.04.2020

Examples of ML

1. Marketing
 - Churn - who is gonna leave
 'https://www.oreilly.com/radar/drivetrain-approach-data-products/
   -1. Most Important define object - Sell more book, etc.
   -2.Define Levers (рычаги) - to get your object' - gifts, call - what things we can do to get object
   Model can help to get object and implement Levers
   - 3. Data - how we can collect
   - 4. Model 5.Interpretation
Most ML not about prediction but about interpretation

2. Risk Application
 - Credit, Fraud
   

    
    
    


