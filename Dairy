My conclusions during coding
12.03.2020

Goal was to write additional block to mass_algo_classif_function, to make Feature Selection base on the feat_importance.
From 8 models, startartize block managed to do only for 3-4 of them.
Reasons: KNN has no feat_importance, CATBoost, Gaussian and SVM.SVC as well required peculiar approach.
Conclusion: 
1. Make separate feature selection after model defined


23.03.2020

Finished and uploaded first draft of Corona_prediction

Future steps:
1. MOre models and tune
2. Check prediction
3. Prediction for more countries
4. Go through another Kernel.Check logic!


30.03

Preventive Maint
1. Score improved by drop y=['ttf'] - 5% min and max
2. Why predicted is beter than av.Std is equal.
Recalculate to RSME and will see

3. Drop colin feat improved score (even best RF) (Done)

4. Graph for Predicted/Actual (Done)

5. Control Outliars by Z_score (Done)

31.03

Instruction for Outliars Detection and Normality Check (Done)

01.04
Binary classification
1. Predict_proba (2 columns - probability to belong one of classes) and 
decision function (if no predict_proba, posotive- class 1, neg - class 0)

2. Why Predict_proba but not just prediction - used to define threshold at which we should switch from 0 to 1


02.04
Multiclass classification
1. Prepare y - transform to binary classificaton - LabelBinaliazer
2. Function with GridSearch to calc y_pred or y_score
3. Metrics calculare
4. Precision-Recall Curve to STUDY!

05.04

Lecture 2

1. Create test set from data with time element - without data Shuffle or random pick up.
Because usually new data is fresh one.

2. %time - if more that 10 sec - too long

3. Bagging - Random Forest - Ensemble of trees. 
Build 10 crappy trees, average them and get 1 good.

Different input for each tree (different subsent or data with replacement - bootstrap)

4. If score on valid is low but we dont know is it because valid test has another dataline or because train is overfitted,
we can solve it using __OUT_OF_BAG__ score.
Idea is we calculate train error on the rows from train set but that wasnt used in this tree fitting.
 So we dont need valid set and we can check how model generalize on the new data
 
 5. RF tunning - min_sample_leaf - stop split node if <=min_sample_leaf
 6. RF tunning - max_features -
 
 Idea to have less correlated trees.
 If you have so good columns, each tree starts with splitting this column.
 With this tune we are looking for splitting not to all columns but to subset of them (50 %, 70%, sqrt, log)
  
  
  
  - stop split node if <=min_sample_leaf


